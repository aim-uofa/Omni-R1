{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "314626d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a47de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/public/home/zhonghao/miniconda3/envs/test/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    Qwen2_5OmniModel,\n",
    "    Qwen2_5OmniProcessor,\n",
    "    GenerationConfig,\n",
    "    Qwen2_5OmniThinkerForConditionalGeneration,\n",
    ")\n",
    "\n",
    "from qwen_omni_utils import process_mm_info\n",
    "\n",
    "\n",
    "omni_r1_path = \"/mnt/public/home/zhonghao/Omini-R1-ORI/train_logs/omni_multi_9k_temper_fused_res_100_h100_4/\"\n",
    "# \"/mnt/public/home/zhonghao/Omini-R1-ORI/train_logs/omni_multi_9k_temper_fused_res_100_h100_4/\"\n",
    "# /mnt/public/home/zhonghao/Omni-R1/train_logs/omni_refactor\n",
    "qwen_omni_path = \"/mnt/public/weight/Qwen2.5-Omni-7B\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0af02b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.41s/it]\n",
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "Qwen2_5OmniToken2WavModel must inference with fp32, but flash_attention_2 only supports fp16 and bf16, attention implementation of Qwen2_5OmniToken2WavModel will fallback to sdpa.\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:11<00:00,  2.37s/it]\n"
     ]
    }
   ],
   "source": [
    "omni_r1 = Qwen2_5OmniThinkerForConditionalGeneration.from_pretrained(\n",
    "    omni_r1_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ").eval()\n",
    "qwen_omni = Qwen2_5OmniModel.from_pretrained(\n",
    "    qwen_omni_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ").thinker.eval()\n",
    "\n",
    "processor = Qwen2_5OmniProcessor.from_pretrained(qwen_omni_path)\n",
    "\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    use_cache=True, max_new_tokens=1024, do_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "def07ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, video_path, prompt, sys_prompt, use_audio_in_video=True):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": sys_prompt}]},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"video\", \"video\": video_path},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    text_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    audio_input, image_input, video_input, process_args = process_mm_info(\n",
    "        messages, use_audio_in_video=use_audio_in_video\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=text_input,\n",
    "        images=image_input,\n",
    "        audios=audio_input,\n",
    "        videos=video_input,\n",
    "        use_audio_in_video=use_audio_in_video,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        padding_side=\"left\",\n",
    "        do_resize=True,\n",
    "    )\n",
    "    inputs = inputs.to(model.device).to(model.dtype)\n",
    "\n",
    "    # 生成输出\n",
    "    with torch.inference_mode():\n",
    "        generated_ids = model.generate(**inputs, use_audio_in_video=use_audio_in_video, generation_config=generation_config)\n",
    "\n",
    "    prompt_length = inputs[\"input_ids\"].size(1)\n",
    "    completion_ids = generated_ids[:, prompt_length:]\n",
    "    # Decode the generated completions\n",
    "    text = processor.batch_decode(completion_ids, skip_special_tokens=True)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11940f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f70c94c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"assets/videos/shopping.mp4\"\n",
    "prompt = (\n",
    "    \"Which kind of drinks is picked up last?\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58f85a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:System prompt modified, audio output may not work as expected. Audio output mode only works when using default system prompt 'You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.'\n",
      "qwen-vl-utils using decord to read video.\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'pad_token_id': 151643, 'bos_token_id': 151644, 'eos_token_id': 151645}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last drink picked up is a white bottle labeled \"维C\" (Vitamin C).\n"
     ]
    }
   ],
   "source": [
    "response = inference(\n",
    "    omni_r1, video_path, prompt=prompt, sys_prompt=\"You are a helpful assistant.\", use_audio_in_video=False\n",
    ")\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1653517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:System prompt modified, audio output may not work as expected. Audio output mode only works when using default system prompt 'You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last drink picked up is a white bottle with a black cap and a label that reads \"维C\" (Vitamin C).\n"
     ]
    }
   ],
   "source": [
    "## Use a local HuggingFace model to inference.\n",
    "response = inference(\n",
    "    qwen_omni, video_path, prompt=prompt, sys_prompt=\"You are a helpful assistant.\", use_audio_in_video=False\n",
    ")\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "259bca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"assets/videos/beef.mp4\"\n",
    "prompt = \"Localize a series of activity events in the video, output the start and end timestamp for each event, and describe each event with sentences. Provide the result in json format with 'mm:ss.ff' format for time depiction.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5710bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:System prompt modified, audio output may not work as expected. Audio output mode only works when using default system prompt 'You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.'\n",
      "/mnt/public/home/zhonghao/open_source/Omni-R1/src/qwen-omni-utils/src/qwen_omni_utils/v2_5/audio_process.py:57: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audios.append(librosa.load(path, sr=16000)[0])\n",
      "/mnt/public/home/zhonghao/miniconda3/envs/test/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38496 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "    {\n",
      "        \"start_time\": \"00:38.00\",\n",
      "        \"end_time\": \"00:46.00\",\n",
      "        \"description\": \"season the brisket with salt and pepper.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"00:46.00\",\n",
      "        \"end_time\": \"00:54.00\",\n",
      "        \"description\": \"place the brisket on the grill.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"00:54.00\",\n",
      "        \"end_time\": \"01:02.00\",\n",
      "        \"description\": \"add some oil to the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"01:02.00\",\n",
      "        \"end_time\": \"01:10.00\",\n",
      "        \"description\": \"add chopped onions carrots and bell peppers to the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"01:10.00\",\n",
      "        \"end_time\": \"01:18.00\",\n",
      "        \"description\": \"add a chicken stock cube to the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"01:18.00\",\n",
      "        \"end_time\": \"01:26.00\",\n",
      "        \"description\": \"place the brisket on the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"01:26.00\",\n",
      "        \"end_time\": \"01:34.00\",\n",
      "        \"description\": \"add some barbecue sauce to the brisket.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"01:34.00\",\n",
      "        \"end_time\": \"01:42.00\",\n",
      "        \"description\": \"add some beer to the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"01:42.00\",\n",
      "        \"end_time\": \"01:50.00\",\n",
      "        \"description\": \"place the pan on the coals.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"01:50.00\",\n",
      "        \"end_time\": \"02:00.00\",\n",
      "        \"description\": \"cook the brisket for 2.5 hours.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"02:00.00\",\n",
      "        \"end_time\": \"02:08.00\",\n",
      "        \"description\": \"remove the brisket from the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"02:08.00\",\n",
      "        \"end_time\": \"02:16.00\",\n",
      "        \"description\": \"place the brisket on a cutting board.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"02:16.00\",\n",
      "        \"end_time\": \"02:24.00\",\n",
      "        \"description\": \"slice the brisket.\"\n",
      "    }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = inference(\n",
    "    omni_r1, video_path, prompt=prompt, sys_prompt=\"You are a helpful assistant.\", use_audio_in_video=True\n",
    ")\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e559fd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:System prompt modified, audio output may not work as expected. Audio output mode only works when using default system prompt 'You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.'\n",
      "/mnt/public/home/zhonghao/open_source/Omni-R1/src/qwen-omni-utils/src/qwen_omni_utils/v2_5/audio_process.py:57: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audios.append(librosa.load(path, sr=16000)[0])\n",
      "/mnt/public/home/zhonghao/miniconda3/envs/test/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "    {\n",
      "        \"start_time\": \"00:37.00\",\n",
      "        \"end_time\": \"00:44.00\",\n",
      "        \"description\": \"cut the fat off the brisket.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"00:45.00\",\n",
      "        \"end_time\": \"00:54.00\",\n",
      "        \"description\": \"rub the brisket with salt and pepper.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"01:00.00\",\n",
      "        \"end_time\": \"01:05.00\",\n",
      "        \"description\": \"add some olive oil to the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"01:06.00\",\n",
      "        \"end_time\": \"01:10.00\",\n",
      "        \"description\": \"add chopped onions to the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"01:11.00\",\n",
      "        \"end_time\": \"01:15.00\",\n",
      "        \"description\": \"add chopped carrots to the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"01:16.00\",\n",
      "        \"end_time\": \"01:20.00\",\n",
      "        \"description\": \"add chopped yellow and red bell peppers to the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"01:21.00\",\n",
      "        \"end_time\": \"01:25.00\",\n",
      "        \"description\": \"add some chopped garlic to the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"01:26.00\",\n",
      "        \"end_time\": \"01:30.00\",\n",
      "        \"description\": \"add some chopped celery to the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"01:31.00\",\n",
      "        \"end_time\": \"01:35.00\",\n",
      "        \"description\": \"add some chopped tomatoes to the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"01:36.00\",\n",
      "        \"end_time\": \"01:40.00\",\n",
      "        \"description\": \"add some chopped mushrooms to the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"01:41.00\",\n",
      "        \"end_time\": \"01:45.00\",\n",
      "        \"description\": \"add some chopped potatoes to the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"01:46.00\",\n",
      "        \"end_time\": \"01:50.00\",\n",
      "        \"description\": \"add some chopped onions to the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"01:51.00\",\n",
      "        \"end_time\": \"01:55.00\",\n",
      "        \"description\": \"add some chopped carrots to the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"01:56.00\",\n",
      "        \"end_time\": \"02:00.00\",\n",
      "        \"description\": \"add some chopped yellow and red bell peppers to the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"02:01.00\",\n",
      "        \"end_time\": \"02:05.00\",\n",
      "        \"description\": \"add some chopped garlic to the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"02:06.00\",\n",
      "        \"end_time\": \"02:10.00\",\n",
      "        \"description\": \"add some chopped celery to the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"02:11.00\",\n",
      "        \"end_time\": \"02:15.00\",\n",
      "        \"description\": \"add some chopped tomatoes to the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"02:16.00\",\n",
      "        \"end_time\": \"02:20.00\",\n",
      "        \"description\": \"add some chopped mushrooms to the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"02:21.00\",\n",
      "        \"end_time\": \"02:25.00\",\n",
      "        \"description\": \"add some chopped potatoes to the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"02:26.00\",\n",
      "        \"end_time\": \"02:30.00\",\n",
      "        \"description\": \"add some chopped onions to the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"02:31.00\",\n",
      "        \"end_time\": \"02:35.00\",\n",
      "        \"description\": \"add some chopped carrots to the pan.\"\n",
      "    },\n",
      "    {\n",
      "        \"start_time\": \"02:36.00\",\n",
      "        \"end_time\": \"02:4\n"
     ]
    }
   ],
   "source": [
    "response = inference(\n",
    "    qwen_omni, video_path, prompt=prompt, sys_prompt=\"You are a helpful assistant.\", use_audio_in_video=True\n",
    ")\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7ec7584",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"assets/videos/refavs_demo.mp4\"\n",
    "prompt = \"Which object(s) you think make the sound in the video? Describe the sound and the object(s) in detail.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "265a9d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:System prompt modified, audio output may not work as expected. Audio output mode only works when using default system prompt 'You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.'\n",
      "/mnt/public/home/zhonghao/open_source/Omni-R1/src/qwen-omni-utils/src/qwen_omni_utils/v2_5/audio_process.py:57: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audios.append(librosa.load(path, sr=16000)[0])\n",
      "/mnt/public/home/zhonghao/miniconda3/envs/test/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sound in the video is produced by a sitar, which is a long-necked lute with a gourd-shaped resonator. The sitar is played by plucking its strings with a small, curved instrument called a mizrab. The sound is characterized by a distinctive, resonant tone that is often associated with Indian classical music.\n"
     ]
    }
   ],
   "source": [
    "response = inference(\n",
    "    omni_r1, video_path, prompt=prompt, sys_prompt=\"You are a helpful assistant.\", use_audio_in_video=True\n",
    ")\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e327e02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:System prompt modified, audio output may not work as expected. Audio output mode only works when using default system prompt 'You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.'\n",
      "/mnt/public/home/zhonghao/open_source/Omni-R1/src/qwen-omni-utils/src/qwen_omni_utils/v2_5/audio_process.py:57: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audios.append(librosa.load(path, sr=16000)[0])\n",
      "/mnt/public/home/zhonghao/miniconda3/envs/test/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sound in the video is produced by a sitar. The sitar is a stringed instrument that is commonly used in Indian classical music. It has a long neck and a gourd-shaped resonator. The sitar player uses a pick to pluck the strings, which produce the sound. The sound of the sitar is characterized by its twangy and resonant quality.\n"
     ]
    }
   ],
   "source": [
    "response = inference(\n",
    "    qwen_omni, video_path, prompt=prompt, sys_prompt=\"You are a helpful assistant.\", use_audio_in_video=True\n",
    ")\n",
    "print(response[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
